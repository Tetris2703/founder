Для сверточного слоя входные данные обычно представлены в виде матрицы (например, изображение). Предположим, что у нас есть входное изображение $\mathbf{X}$ размером $4 \times 4$ и фильтр $\mathbf{W}$ размером $2 \times 2$.

$$
\mathbf{X} = \begin{pmatrix}
x_{11} & x_{12} & x_{13} & x_{14} \\
x_{21} & x_{22} & x_{23} & x_{24} \\
x_{31} & x_{32} & x_{33} & x_{34} \\
x_{41} & x_{42} & x_{43} & x_{44}
\end{pmatrix}
$$

$$
\mathbf{W} = \begin{pmatrix}
w_{11} & w_{12} \\
w_{21} & w_{22}
\end{pmatrix}
$$

Свертка выполняется следующим образом:

$$
E_{ij} = \sum_{m=0}^{1} \sum_{n=0}^{1} X_{i+m, j+n} W_{m+1, n+1}
$$

Например, для первого элемента выходного изображения:

$$
E_{11} = x_{11} w_{11} + x_{12} w_{12} + x_{21} w_{21} + x_{22} w_{22}
$$

Затем применяется активационная функция $\sigma$:

$$
y_{ij} = \sigma(E_{ij})
$$

### 3. Рекуррентный слой (Recurrent Layer)

Для рекуррентного слоя, такого как LSTM, входные данные обрабатываются последовательно. Предположим, что у нас есть последовательность $\mathbf{x} = [x_1, x_2, x_3, x_4]$ и скрытое состояние $\mathbf{h}$. Для каждого временного шага $t$:

$$
\mathbf{h}_t = \sigma(\mathbf{W}_h \mathbf{h}_{t-1} + \mathbf{W}_x \mathbf{x}_t + \mathbf{b})
$$

Где $\mathbf{W}_h$ и $\mathbf{W}_x$ — матрицы весов, а $\mathbf{b}$ — вектор смещений.

### 4. Слой нормализации (Normalization Layer)

Для слоя нормализации, такого как BatchNormalization, входные данные нормализуются.

$$
\mu = \frac{1}{N} \sum_{i=1}^{N} x_i
$$

$$
\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2
$$

$$
\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$

Где $\mu$ — среднее значение, $\sigma^2$ — дисперсия, $\epsilon$ — малое число для стабильности. Затем применяется линейное преобразование:

$$
y_i = \gamma \hat{x}_i + \beta
$$

Где $\gamma$ и $\beta$ — параметры масштабирования и смещения.

### 5. Слой пулинга (Pooling Layer)

Для слоя пулинга, такого как MaxPooling2D, входные данные разбиваются на окна, и для каждого окна выбирается максимальное значение. Предположим, что у нас есть входное изображение $\mathbf{X}$ размером $4 \times 4$ и окно размером $2 \times 2$.

$$
\mathbf{X} = \begin{pmatrix}
x_{11} & x_{12} & x_{13} & x_{14} \\
x_{21} & x_{22} & x_{23} & x_{24} \\
x_{31} & x_{32} & x_{33} & x_{34} \\
x_{41} & x_{42} & x_{43} & x_{44}
\end{pmatrix}
$$

Для первого окна:

$$
E_{11} = \max(x_{11}, x_{12}, x_{21}, x_{22})
$$

### 6. Слой Dropout

Для слоя Dropout часть входных данных случайным образом обнуляется. Предположим, что у нас есть вектор $\mathbf{x} = [x_1, x_2, x_3, x_4]$ и вероятность обнуления $p$. Для каждого элемента $x_i$:

$$
\hat{x}_i = \begin{cases}
0 & \text{с вероятностью } p \\
\frac{x_i}{1-p} & \text{с вероятностью } 1-p
\end{cases}
$$

Надеюсь, эти примеры помогли вам лучше понять, как различные типы слоев обрабатывают входные данные!
