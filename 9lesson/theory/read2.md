## 1. Разделение на главы

- Введение в фреймворки глубокого обучения.
- Концепция многослойного персептрона (MLP).
- Загрузка и подготовка данных.
- Построение нейронной сети на основе TensorFlow.
- Оптимизация и алгоритмы обучения.
- Оценка производительности модели.
- Сохранение и использование модели.
## 2. Термины и определения
К каждой главе выпишу ключевые термины и объясню их простыми словами:

### 1. Введение в фреймворки глубокого обучения
Фреймворк: Инструмент для работы с нейронными сетями. Примеры: TensorFlow, PyTorch.
Градиентный спуск: Метод оптимизации для обучения моделей, который корректирует параметры, чтобы минимизировать ошибку.
Theano, TensorFlow, PyTorch: Разные программные библиотеки для работы с нейронными сетями.
### 2. Концепция многослойного персептрона (MLP)
Персептрон: Самая простая модель нейрона, которая умножает входные данные на веса, добавляет смещение и выдает результат.
Полносвязный слой (Dense Layer): Слой нейронной сети, где каждый выход одного слоя связан со всеми входами следующего.
Функция активации (ReLU, Softmax): Преобразует выходные данные, чтобы нейрон "реагировал" на сложные ситуации.
### 3. Загрузка и подготовка данных
MNIST: Набор данных с изображениями рукописных цифр.
Batch (пакет данных): Небольшая порция данных, используемая для обучения модели за один раз.
Предобработка: Подготовка данных (например, нормализация значений, изменение формы).
### 4. Построение нейронной сети на основе TensorFlow
TensorFlow: Библиотека для машинного обучения и глубокого обучения.
Класс (Class): Способ упрощения работы, который объединяет данные и функции для построения нейронных сетей.
Xavier инициализация: Метод задания начальных значений весов, чтобы сеть обучалась лучше.
### 5. Оптимизация и алгоритмы обучения
Adam: Один из самых популярных алгоритмов для настройки весов сети.
Градиенты: Изменения параметров, которые помогают модели становиться лучше.
GradientTape: Инструмент в TensorFlow для вычисления производных (градиентов).
### 6. Оценка производительности модели
Функция потерь: Показывает, насколько модель ошибается (например, перекрестная энтропия).
Точность (Accuracy): Доля правильных предсказаний модели.
Матрица ошибок (Confusion Matrix): Таблица, показывающая, какие классы модель путает.
### 7. Сохранение и использование модели
Экспорт модели: Процесс сохранения обученной модели для повторного использования.
Checkpoint: Точка сохранения параметров модели.
TensorFlow SavedModel: Формат для сохранения модели вместе со всеми параметрами и графиками вычислений.


##  Пересказ методички, структурированный по главам

### 1. Введение в фреймворки глубокого обучения
Современные нейронные сети разрабатываются с использованием специальных библиотек, называемых фреймворками. 
Среди них выделяются TensorFlow и PyTorch, которые пришли на смену устаревшим библиотекам вроде Theano. 
Эти фреймворки позволяют легко строить, обучать и тестировать сложные модели. Например, TensorFlow удобен за счет 
своей надежности и гибкости, а PyTorch популярен благодаря простоте и поддержке динамических вычислений.

### 2. Концепция многослойного персептрона (MLP)
Многослойный персептрон (MLP) — это базовая модель нейронной сети. Она состоит из нескольких полносвязных слоев, 
где каждый выход одного слоя полностью подключен ко входу следующего. Основные элементы:

Персептрон: Умножает входные данные на веса, добавляет смещение и применяет функцию активации.
Функции активации: 
#### Примеры:
- ReLU (возвращает 0 для отрицательных значений и само значение для положительных).
- Softmax (преобразует числа в вероятности для многоклассовой классификации).
### 3. Загрузка и подготовка данных
Для примера используется набор данных MNIST — изображения рукописных цифр с метками (какая цифра изображена). Д
анные разделяются на три части:

- Обучающий набор — для тренировки модели.
- Валидационный — для проверки во время обучения.
- Тестовый — для окончательной оценки.
- Данные предварительно подготавливаются:

Преобразование изображений в одномерные массивы.
Масштабирование значений пикселей в диапазон от 0 до 1 для ускорения и улучшения обучения.
### 4. Построение нейронной сети на основе TensorFlow
Сеть строится с использованием библиотеки TensorFlow. Основные этапы:

Создание слоев: Каждый слой имеет веса (настраиваемые параметры) и функцию активации.
Инициализация весов: Используется метод Xavier, чтобы избежать слишком больших или маленьких значений на старте.
Сборка модели: Например, MLP может включать входной слой с 784 признаками, два скрытых слоя (700 и 500 нейронов)
с функцией ReLU и выходной слой с 10 классами (Softmax).

### 5. Оптимизация и алгоритмы обучения
Для обучения модели используется метод градиентного спуска. В нашем случае используется Adam — улучшенный алгоритм, 
который автоматически регулирует шаги обучения, что ускоряет процесс и делает его стабильнее. Градиенты вычисляются 
с помощью инструмента GradientTape, который автоматически находит производные для всех параметров модели.

### 6. Оценка производительности модели
После обучения модель оценивают:

Функция потерь: Используется перекрестная энтропия, которая измеряет, насколько хорошо предсказания модели 
совпадают с реальными метками.
Точность: Доля правильных предсказаний.
Матрица ошибок: Таблица, которая показывает, какие цифры модель путает и как часто.
В методичке показано, как визуализировать результаты с помощью графиков ошибок и точности.

### 7. Сохранение и использование модели
После обучения модель можно сохранить в формате TensorFlow SavedModel. Это позволяет:

Загружать модель для использования в других проектах.
Выполнять прогнозы на новых данных без необходимости повторного обучения.
Пример задачи
Модель тренируется на наборе MNIST для классификации рукописных цифр. После 10 эпох обучения достигается 
высокая точность (более 97%). Это значит, что сеть хорошо распознает цифры, но иногда путается с похожими, 
например, 8 и 3. Чтобы улучшить качество, можно использовать больше данных или настроить параметры модели.
