# Активационные функции

Активационные функции играют важную роль в нейронных сетях, позволяя моделям обучаться сложным паттернам. В этом документе представлены несколько распространенных активационных функций.

## 1. Функция Хевисайда

Функция Хевисайда определена как:

$$
H(x) = 
\begin{cases} 
0, & x < 0 \\
1, & x \geq 0 
\end{cases}
$$

### Свойства:
- Дискретная функция, которая возвращает 0 для отрицательных значений и 1 для нулевых и положительных.
- Используется в задачах классификации.

## 2. Сигмоида

Сигмоидная функция имеет вид:

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

### Свойства:
- Выходные значения находятся в диапазоне (0, 1).
- Часто используется в выходном слое для бинарной классификации.
- Имеет гладкую производную, но страдает от проблемы исчезающего градиента.

## 3. Гиперболический тангенс

Гиперболический тангенс определяется как:

$$
tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
$$

### Свойства:
- Выходные значения находятся в диапазоне (-1, 1).
- Более центрированная функция по сравнению с сигмоидой, что может ускорить обучение.
- Также подвержена проблеме исчезающего градиента.

## 4. ReLU (Rectified Linear Unit)

Функция ReLU задается следующим образом:

$$
{ReLU}(x) = max(0, x)
$$

### Свойства:
- Простой и эффективный способ активации.
- Помогает избежать проблемы исчезающего градиента.
- Может привести к "мертвым" нейронам, если они никогда не активируются.

## 5. Leaky ReLU

Leaky ReLU — это модификация ReLU:

$$
{Leaky ReLU}(x) = 
\begin{cases} 
x, & x > 0 \\
\alpha x, & x \leq 0 
\end{cases}
$$

где $\alpha$ — малый положительный коэффициент (обычно $0.01$).

### Свойства:
- Позволяет некоторым отрицательным значениям проходить через функцию.
- Уменьшает вероятность "мертвых" нейронов по сравнению с обычным ReLU.

## 6. ELU (Exponential Linear Unit)

ELU определяется как:

$$
{ELU}(x) = 
\begin{cases} 
x, & x > 0 \\
\alpha (e^{x} - 1), & x \leq 0 
\end{cases}
$$

где $\alpha$ — положительный параметр.

### Свойства:
- Стремится к нулю для отрицательных значений, что может помочь уменьшить смещение.
- Гладкая производная, что улучшает обучение.

## Заключение

Каждая из этих активационных функций имеет свои преимущества и недостатки. Выбор подходящей функции зависит от конкретной задачи и архитектуры модели.
